{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tele_df = pd.read_excel('telestrations-data.xlsx', sheet_name='anon')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding position column to count order using the index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Transformation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding a position column\n",
    "The position column represents the order that notebooks follow (assuming that the natural order of the rows in the data corresponds with the Telestrations notebook path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df['Position'] = tele_df.reset_index()['index']\n",
    "tele_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Melting the data for better usability"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "melted_tele_df = pd.melt(tele_df, value_vars=[1,2,3,4,5], value_name='Prompt',var_name='Round', id_vars=['Names','Position'])\n",
    "melted_tele_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our 'Names' column is a little misleading. Our names really just corresponds to whoever started with the notebook, so we'll rename this column to 'Notebook'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename name to represent the notebook\n",
    "melted_tele_df = melted_tele_df.rename(columns={'Names':'Notebook' })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding columns to support our comparisons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the Drawer of 'Prompt'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_position = melted_tele_df['Position'].max()\n",
    "\n",
    "# Getting the 'Position' of the person that drew 'Prompt'\n",
    "melted_tele_df['drawer_position'] = melted_tele_df.apply(lambda row: ((row['Position'] + 1 + 2*(row['Round']-1)) % (max_position+1)), axis=1)\n",
    "\n",
    "# Getting the actual name of the person that drew 'Prompt'\n",
    "melted_tele_df['Drawer'] = melted_tele_df.apply(lambda row:\n",
    "    melted_tele_df[(melted_tele_df['Position'] == row['drawer_position']) & (melted_tele_df['Round'] == 1)]['Notebook'].iloc[0], axis=1)\n",
    "melted_tele_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the Guesser of the drawing of 'Prompt'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the 'Position' of the person that guessed the drawer's rendition of 'Prompt'\n",
    "melted_tele_df['guesser_position'] = melted_tele_df.apply(lambda row: ((row['Position'] + 2 + 2*(row['Round']-1)) % (max_position+1)), axis=1)\n",
    "# Getting the actual of the person that guessed the drawer's rendition of 'Prompt'\n",
    "melted_tele_df['Guesser'] = melted_tele_df.apply(lambda row: melted_tele_df[(melted_tele_df['Position']==row['guesser_position']) & (melted_tele_df['Round'] == 1)]['Notebook'].iloc[0], axis=1)\n",
    "melted_tele_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "melted_tele_df.sort_values(by=['Position','Round'],ascending=[False,True])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the Guesser's Guess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_round = melted_tele_df['Round'].max()\n",
    "\n",
    "melted_tele_df['Next Round'] = melted_tele_df.apply(lambda row: row['Round'] + 1 if row['Round'] < max_round else -1, axis=1)\n",
    "\n",
    "melted_tele_df['Guess'] = melted_tele_df.apply(lambda row: melted_tele_df[\n",
    "    (melted_tele_df['Round'] == row['Next Round']) & (melted_tele_df['Notebook'] == row['Notebook'])\n",
    "]['Prompt'].iloc[0] if row['Next Round'] != -1 else '', axis=1)\n",
    "melted_tele_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! Now we have the necessary data to make our calculations. Before we do that we're first going to create a DataFrame that removes unnecessary columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Removing 's from text\n",
    "melted_tele_df['Prompt'] = melted_tele_df['Prompt'].apply(lambda x: x.replace(\"'\",\"\").lower())\n",
    "melted_tele_df['Guess'] = melted_tele_df['Guess'].apply(lambda x: x.replace(\"'\",\"\").lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df_clean = melted_tele_df[['Notebook','Round','Drawer','Guesser','Prompt','Guess']]\n",
    "tele_df_clean = tele_df_clean[tele_df_clean['Round'] < max_round]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finding the similarity between prompts and guesses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import embeddings\n",
    "embeddings.add_embeddings(tele_df_clean['Prompt'].unique().tolist())\n",
    "embeddings.add_embeddings(tele_df_clean['Guess'].unique().tolist())\n",
    "prompt_embeddings = embeddings.load_embeddings()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df_clean['cosine_similarity'] = tele_df_clean.apply(lambda row: embeddings.get_cosine_similarity(prompt_embeddings[row['Prompt']],prompt_embeddings[row['Guess']]), axis=1)\n",
    "tele_df_clean.sort_values('cosine_similarity')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df_clean['cosine_similarity'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scoring"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that bad guesses still land at around 0.75 cosine similarity, so when creating this new scale we're going to set 0.75 as the minimum score and 1 as the maximum score, and then set this on a range from 1 to 10."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Scoring"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Linear score\n",
    "floor = 0.75\n",
    "max_score = 10\n",
    "\n",
    "tele_df_clean['Linear_Score'] = tele_df_clean['cosine_similarity'].apply(lambda x: max_score * ((x - floor) / (1 - floor)) if x > floor else 0)\n",
    "\n",
    "sns.jointplot(x=tele_df_clean['cosine_similarity'], y=tele_df_clean['Linear_Score'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logarithmic Scoring"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Logarithmic score\n",
    "\n",
    "floor = 0.75\n",
    "max_score = 10\n",
    "\n",
    "def compute_a(b):\n",
    "    return 10 / np.log(b * 0.25 + 1)\n",
    "\n",
    "b = 50\n",
    "a = compute_a(b)\n",
    "\n",
    "tele_df_clean['Log_Score'] = tele_df_clean['cosine_similarity'].apply(lambda x: a * np.log(b * (x - floor) + 1) if x > floor else 0)\n",
    "sns.jointplot(x=tele_df_clean['cosine_similarity'], y=tele_df_clean['Log_Score'])\n",
    "tele_df_clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Scoring"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "floor = 0.75\n",
    "max_score = 10\n",
    "k = 40  # Adjust for steeper or more gradual transitions\n",
    "x_0 = 0.875  # Midpoint\n",
    "\n",
    "def logistic(x):\n",
    "    return max_score / (1 + np.exp(-k*(x - x_0)))\n",
    "\n",
    "def normalized_score(x, floor=0.75, max_val=1, max_score=10):\n",
    "    if (x < floor):\n",
    "        return 0\n",
    "    else:\n",
    "        return (logistic(x) - logistic(floor)) / (logistic(max_val) - logistic(floor)) * max_score\n",
    "\n",
    "def compute_score(x):\n",
    "    min_val = logistic(floor)\n",
    "    max_val = logistic(1)\n",
    "    return normalized_score(x, min_val, max_val)\n",
    "\n",
    "tele_df_clean['Logistic_Score'] = tele_df_clean['cosine_similarity'].apply(compute_score)\n",
    "tele_df_clean['Prompt->Guess'] = tele_df_clean.apply(lambda row: f'{row[\"Prompt\"]}->{row[\"Guess\"]}', axis=1)\n",
    "tele_df_clean['Drawer->Guesser'] = tele_df_clean.apply(lambda row: f'{row[\"Drawer\"]}->{row[\"Guesser\"]}', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.jointplot(x=tele_df_clean['cosine_similarity'], y=tele_df_clean['Logistic_Score'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the scatter plot\n",
    "ax = sns.scatterplot(x=tele_df_clean['cosine_similarity'],\n",
    "                     y=tele_df_clean['Logistic_Score'],\n",
    "                     hue=tele_df_clean['Drawer->Guesser'],\n",
    "                     palette='hls')\n",
    "\n",
    "# Get the PathCollection which represents the data points in scatterplot\n",
    "path_collection = ax.collections[0]\n",
    "\n",
    "# Extract the colors from the PathCollection\n",
    "colors = path_collection.get_facecolor()\n",
    "\n",
    "texts = []\n",
    "for line in range(0, tele_df_clean.shape[0]):\n",
    "    texts.append(plt.text(tele_df_clean['cosine_similarity'].iloc[line],\n",
    "                          tele_df_clean['Logistic_Score'].iloc[line],\n",
    "                          tele_df_clean['Prompt->Guess'].iloc[line],\n",
    "                          horizontalalignment='left',\n",
    "                          size='small',\n",
    "                          color=colors[line]))\n",
    "\n",
    "# Adjust text to minimize overlaps\n",
    "adjust_text(texts)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from adjustText import adjust_text\n",
    "#\n",
    "# # Adjust the figure size\n",
    "# plt.figure(figsize=(15, 10))\n",
    "#\n",
    "# # Create the scatter plot\n",
    "# ax = sns.scatterplot(x=tele_df_clean['cosine_similarity'],\n",
    "#                      y=tele_df_clean['Logistic_Score'],\n",
    "#                      hue=tele_df_clean['Drawer'])\n",
    "#\n",
    "# texts = []\n",
    "# for line in range(0, tele_df_clean.shape[0]):\n",
    "#     texts.append(ax.text(tele_df_clean['cosine_similarity'].iloc[line],\n",
    "#                          tele_df_clean['Logistic_Score'].iloc[line],\n",
    "#                          tele_df_clean['Prompt->Guess'].iloc[line],\n",
    "#                          horizontalalignment='left',\n",
    "#                          size='small',\n",
    "#                          color='black'))\n",
    "#\n",
    "# # Adjust text to minimize overlaps\n",
    "# adjust_text(texts)\n",
    "#\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tele_df_final = tele_df_clean.drop(columns=['Linear_Score','Log_Score']).rename(columns={'Logistic_Score':'Score'})\n",
    "tele_df_final.sort_values(by=['Notebook','Round'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Scoreboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drawing_guessing_weight = 2\n",
    "\n",
    "# Get series with total drawer/guesser scores by person\n",
    "drawer_scores = tele_df_final.groupby('Drawer')['Score'].sum()\n",
    "guesser_scores = tele_df_final.groupby('Guesser')['Score'].sum()\n",
    "\n",
    "# merge scores\n",
    "all_scores = pd.merge(drawer_scores.to_frame().reset_index(), guesser_scores.to_frame().reset_index(), left_on='Drawer',right_on='Guesser',suffixes=['_Drawing','_Guessing'])\n",
    "\n",
    "# Drop one of the names and rename the other to 'Name'\n",
    "all_scores = all_scores.drop(columns='Guesser').rename(columns={'Drawer':'Name'})\n",
    "\n",
    "# # Calculate weighted scores based on drawing_guessing_weight. With that set to 2 we value drawing twice as much as guessing\n",
    "all_scores['Score_Drawing'] = (all_scores['Score_Drawing'] * drawing_guessing_weight) / (1 + drawing_guessing_weight)\n",
    "all_scores['Score_Guessing'] = all_scores['Score_Guessing'] / (1 + drawing_guessing_weight)\n",
    "all_scores['Composite_Score'] = all_scores['Score_Drawing'] + all_scores['Score_Guessing']\n",
    "all_scores\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_scores = all_scores.sort_values(by='Composite_Score')\n",
    "\n",
    "ax = sns.barplot(x=all_scores['Name'], y=all_scores['Composite_Score'])\n",
    "\n",
    "# Add chart title\n",
    "ax.set_title('Telestrations Scoreboard')\n",
    "\n",
    "# Set x-axis label rotation to vertical\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "sns.barplot(x=all_scores['Name'], y=all_scores['Composite_Score']);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing Notebooks Deviation Round over Round"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To do this we need to first pull in the original prompt into the dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis_df = tele_df_final\n",
    "analysis_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pull in original prompt\n",
    "analysis_df['Original Prompt'] = analysis_df.apply(lambda row: analysis_df[\n",
    "    (analysis_df['Notebook'] == row['Notebook']) & (analysis_df['Round'] == 1)]['Prompt'].iloc[0], axis=1)\n",
    "analysis_df.sort_values(by='Notebook')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "analysis_df['cosine_sim_w_original'] = analysis_df.apply(lambda row: embeddings.get_cosine_similarity(prompt_embeddings[row['Original Prompt']],prompt_embeddings[row['Guess']]), axis=1)\n",
    "analysis_df.sort_values('Notebook')\n",
    "ax = sns.lineplot(x=analysis_df['Round'],y=analysis_df['cosine_sim_w_original'],hue=analysis_df['Notebook'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Annotate the lines\n",
    "lines = ax.get_lines()\n",
    "labels = analysis_df['Notebook'].unique()\n",
    "\n",
    "for line, label in zip(lines, labels):\n",
    "    y = line.get_ydata()[-1]\n",
    "    x = line.get_xdata()[-1]\n",
    "    ax.text(x, y, f'{label}', color=line.get_color(), weight='bold', verticalalignment='center')\n",
    "ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(data=analysis_df, x='Round', y='cosine_sim_w_original', palette='rocket')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(data=analysis_df, x='Round', y='cosine_similarity', palette='rocket')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis_df[analysis_df['Notebook'] == 'Tyler']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Round over round cosine dissimilarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cosine dissimilarity to starting prompt by round"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next steps to explore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quantifying how hard a prompt is to draw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
